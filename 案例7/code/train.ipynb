{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch import nn\n",
    "from easydict import EasyDict as edict\n",
    "from models import Generator, Discriminator, TruncatedVGG19\n",
    "from datasets import SRDataset\n",
    "from utils import *\n",
    "from solver import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# config\n",
    "config = edict()\n",
    "config.csv_folder = '../data/SRDataset'\n",
    "config.HR_data_folder = '../data/SRDataset/div2k/DIV2K_train_HR'\n",
    "config.LR_data_folder = '../data/SRDataset/div2k/DIV2K_train_LR_bicubic/X4'\n",
    "config.crop_size = 96\n",
    "config.scaling_factor = 4\n",
    "\n",
    "# Generator parameters\n",
    "config.G = edict()\n",
    "config.G.large_kernel_size = 9\n",
    "config.G.small_kernel_size = 3\n",
    "config.G.n_channels = 64\n",
    "config.G.n_blocks = 16\n",
    "\n",
    "# Discriminator parameters\n",
    "config.D = edict()\n",
    "config.D.kernel_size = 3\n",
    "config.D.n_channels = 64\n",
    "config.D.n_blocks = 8\n",
    "config.D.fc_size = 1024\n",
    "\n",
    "# Learning parameters\n",
    "config.checkpoint = None # path to model (SRGAN) checkpoint, None if none\n",
    "config.batch_size = 16\n",
    "config.start_epoch = 0\n",
    "config.epochs = 2\n",
    "config.workers = 4\n",
    "config.vgg19_i = 5  # the index i in the definition for VGG loss; see paper\n",
    "config.vgg19_j = 4  # the index j in the definition for VGG loss; see paper\n",
    "config.beta = 1e-3  # the coefficient to weight the adversarial loss in the perceptual loss\n",
    "config.print_freq = 50\n",
    "config.lr = 1e-4\n",
    "\n",
    "# Default device\n",
    "# config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config.device = \"mps\"\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaoxiong.yang/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/chaoxiong.yang/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TruncatedVGG19(\n",
       "  (truncated_vgg19): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truncated VGG19 network to be used in the loss calculation\n",
    "truncated_vgg19 = TruncatedVGG19(i=config.vgg19_i, j=config.vgg19_j)\n",
    "truncated_vgg19.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.checkpoint is None:\n",
    "    # Generator\n",
    "    generator = Generator()\n",
    "\n",
    "    # Initialize generator's optimizer\n",
    "    optimizer_g = torch.optim.SGD(params=filter(lambda p: p.requires_grad, generator.parameters()),\n",
    "                                   lr=config.lr, momentum=0.8)\n",
    "\n",
    "    # Discriminator\n",
    "    discriminator = Discriminator()\n",
    "    optimizer_d = torch.optim.SGD(params=filter(lambda p: p.requires_grad, discriminator.parameters()),\n",
    "                                   lr=config.lr, momentum=0.8)\n",
    "\n",
    "else:\n",
    "    checkpoint = torch.load(config.checkpoint)\n",
    "    config.start_epoch = checkpoint['epoch'] + 1\n",
    "    generator = checkpoint['generator']\n",
    "    discriminator = checkpoint['discriminator']\n",
    "    optimizer_g = checkpoint['optimizer_g']\n",
    "    optimizer_d = checkpoint['optimizer_d']\n",
    "    print(\"\\nLoaded checkpoint from epoch %d.\\n\" % (checkpoint['epoch'] + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "content_loss_criterion = nn.MSELoss()\n",
    "adversarial_loss_criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Move to default device\n",
    "generator = generator.to(config.device)\n",
    "discriminator = discriminator.to(config.device)\n",
    "truncated_vgg19 = truncated_vgg19.to(config.device)\n",
    "content_loss_criterion = content_loss_criterion.to(config.device)\n",
    "adversarial_loss_criterion = adversarial_loss_criterion.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Custom dataloaders\n",
    "train_dataset = SRDataset(split='train', config=config)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=config.batch_size,\n",
    "                                           shuffle=True, \n",
    "                                           num_workers=config.workers,\n",
    "                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/5000]----Batch Time 3.132 (3.132)----Data Time 1.726 (1.726)----Cont. Loss 0.4878 (0.4878)----Adv. Loss 0.6719 (0.6719)----Disc. Loss 1.4287 (1.4287)\n",
      "Epoch: [0][50/5000]----Batch Time 0.701 (0.747)----Data Time 0.000 (0.034)----Cont. Loss 0.4172 (0.4619)----Adv. Loss 0.9919 (0.8379)----Disc. Loss 0.9982 (1.2050)\n",
      "Epoch: [0][100/5000]----Batch Time 0.689 (0.721)----Data Time 0.000 (0.018)----Cont. Loss 0.5545 (0.4507)----Adv. Loss 1.2362 (0.9899)----Disc. Loss 0.7733 (1.0284)\n",
      "Epoch: [0][150/5000]----Batch Time 0.701 (0.711)----Data Time 0.000 (0.012)----Cont. Loss 0.4524 (0.4498)----Adv. Loss 1.6291 (1.1531)----Disc. Loss 0.4719 (0.8766)\n",
      "Epoch: [0][200/5000]----Batch Time 0.688 (0.706)----Data Time 0.000 (0.009)----Cont. Loss 0.2751 (0.4485)----Adv. Loss 2.0575 (1.3165)----Disc. Loss 0.2821 (0.7547)\n",
      "Epoch: [0][250/5000]----Batch Time 0.704 (0.704)----Data Time 0.000 (0.007)----Cont. Loss 0.3101 (0.4463)----Adv. Loss 2.2950 (1.4722)----Disc. Loss 0.2161 (0.6592)\n",
      "Epoch: [0][300/5000]----Batch Time 0.686 (0.702)----Data Time 0.000 (0.006)----Cont. Loss 0.5220 (0.4440)----Adv. Loss 2.3986 (1.6174)----Disc. Loss 0.1935 (0.5836)\n",
      "Epoch: [0][350/5000]----Batch Time 0.689 (0.701)----Data Time 0.000 (0.005)----Cont. Loss 0.3933 (0.4429)----Adv. Loss 2.6253 (1.7498)----Disc. Loss 0.1521 (0.5236)\n",
      "Epoch: [0][400/5000]----Batch Time 0.693 (0.700)----Data Time 0.000 (0.005)----Cont. Loss 0.4374 (0.4415)----Adv. Loss 2.8170 (1.8702)----Disc. Loss 0.1197 (0.4751)\n",
      "Epoch: [0][450/5000]----Batch Time 0.696 (0.699)----Data Time 0.000 (0.004)----Cont. Loss 0.3865 (0.4407)----Adv. Loss 2.9323 (1.9799)----Disc. Loss 0.1032 (0.4353)\n",
      "Epoch: [0][500/5000]----Batch Time 0.687 (0.699)----Data Time 0.000 (0.004)----Cont. Loss 0.4027 (0.4411)----Adv. Loss 2.9955 (2.0792)----Disc. Loss 0.1041 (0.4019)\n",
      "Epoch: [0][550/5000]----Batch Time 0.701 (0.698)----Data Time 0.000 (0.004)----Cont. Loss 0.4147 (0.4396)----Adv. Loss 3.1407 (2.1699)----Disc. Loss 0.0816 (0.3738)\n",
      "Epoch: [0][600/5000]----Batch Time 0.689 (0.697)----Data Time 0.000 (0.003)----Cont. Loss 0.3065 (0.4390)----Adv. Loss 3.2900 (2.2547)----Disc. Loss 0.0673 (0.3496)\n",
      "Epoch: [0][650/5000]----Batch Time 0.692 (0.697)----Data Time 0.000 (0.003)----Cont. Loss 0.3618 (0.4371)----Adv. Loss 3.3544 (2.3343)----Disc. Loss 0.0691 (0.3283)\n",
      "Epoch: [0][700/5000]----Batch Time 0.692 (0.697)----Data Time 0.000 (0.003)----Cont. Loss 0.3147 (0.4385)----Adv. Loss 3.4308 (2.4087)----Disc. Loss 0.0601 (0.3096)\n",
      "Epoch: [0][750/5000]----Batch Time 0.685 (0.697)----Data Time 0.000 (0.003)----Cont. Loss 0.4179 (0.4386)----Adv. Loss 3.5232 (2.4777)----Disc. Loss 0.0533 (0.2931)\n",
      "Epoch: [0][800/5000]----Batch Time 0.683 (0.696)----Data Time 0.000 (0.003)----Cont. Loss 0.3710 (0.4386)----Adv. Loss 3.5868 (2.5425)----Disc. Loss 0.0535 (0.2784)\n",
      "Epoch: [0][850/5000]----Batch Time 0.696 (0.696)----Data Time 0.000 (0.002)----Cont. Loss 0.6473 (0.4392)----Adv. Loss 3.5779 (2.6036)----Disc. Loss 0.0583 (0.2652)\n",
      "Epoch: [0][900/5000]----Batch Time 0.694 (0.696)----Data Time 0.000 (0.002)----Cont. Loss 0.4444 (0.4380)----Adv. Loss 3.7101 (2.6625)----Disc. Loss 0.0473 (0.2532)\n",
      "Epoch: [0][950/5000]----Batch Time 0.691 (0.696)----Data Time 0.000 (0.002)----Cont. Loss 0.2914 (0.4374)----Adv. Loss 3.7418 (2.7181)----Disc. Loss 0.0448 (0.2424)\n",
      "Epoch: [0][1000/5000]----Batch Time 0.690 (0.696)----Data Time 0.000 (0.002)----Cont. Loss 0.4403 (0.4377)----Adv. Loss 3.7728 (2.7706)----Disc. Loss 0.0431 (0.2325)\n",
      "Epoch: [0][1050/5000]----Batch Time 0.688 (0.695)----Data Time 0.000 (0.002)----Cont. Loss 0.2901 (0.4369)----Adv. Loss 3.8473 (2.8204)----Disc. Loss 0.0396 (0.2234)\n",
      "Epoch: [0][1100/5000]----Batch Time 0.690 (0.695)----Data Time 0.001 (0.002)----Cont. Loss 0.2761 (0.4360)----Adv. Loss 3.8905 (2.8685)----Disc. Loss 0.0399 (0.2151)\n",
      "Epoch: [0][1150/5000]----Batch Time 0.691 (0.695)----Data Time 0.000 (0.002)----Cont. Loss 0.4388 (0.4365)----Adv. Loss 3.9298 (2.9143)----Disc. Loss 0.0379 (0.2074)\n",
      "Epoch: [0][1200/5000]----Batch Time 0.692 (0.695)----Data Time 0.000 (0.002)----Cont. Loss 0.3606 (0.4353)----Adv. Loss 3.9624 (2.9585)----Disc. Loss 0.0361 (0.2003)\n",
      "Epoch: [0][1250/5000]----Batch Time 0.690 (0.695)----Data Time 0.000 (0.002)----Cont. Loss 0.3440 (0.4342)----Adv. Loss 4.0696 (3.0008)----Disc. Loss 0.0313 (0.1936)\n",
      "Epoch: [0][1300/5000]----Batch Time 0.702 (0.695)----Data Time 0.000 (0.002)----Cont. Loss 0.5665 (0.4338)----Adv. Loss 4.0568 (3.0415)----Disc. Loss 0.0322 (0.1875)\n",
      "Epoch: [0][1350/5000]----Batch Time 0.692 (0.695)----Data Time 0.000 (0.002)----Cont. Loss 0.4058 (0.4334)----Adv. Loss 4.0887 (3.0806)----Disc. Loss 0.0343 (0.1817)\n",
      "Epoch: [0][1400/5000]----Batch Time 0.683 (0.695)----Data Time 0.000 (0.002)----Cont. Loss 0.6718 (0.4327)----Adv. Loss 4.1468 (3.1184)----Disc. Loss 0.0317 (0.1763)\n",
      "Epoch: [0][1450/5000]----Batch Time 0.694 (0.694)----Data Time 0.000 (0.002)----Cont. Loss 0.3948 (0.4332)----Adv. Loss 4.1845 (3.1549)----Disc. Loss 0.0300 (0.1712)\n",
      "Epoch: [0][1500/5000]----Batch Time 0.693 (0.694)----Data Time 0.000 (0.002)----Cont. Loss 0.7149 (0.4331)----Adv. Loss 4.1342 (3.1904)----Disc. Loss 0.0321 (0.1665)\n",
      "Epoch: [0][1550/5000]----Batch Time 0.692 (0.694)----Data Time 0.000 (0.002)----Cont. Loss 0.6807 (0.4329)----Adv. Loss 4.2370 (3.2248)----Disc. Loss 0.0269 (0.1620)\n",
      "Epoch: [0][1600/5000]----Batch Time 0.687 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.3407 (0.4328)----Adv. Loss 4.3440 (3.2581)----Disc. Loss 0.0235 (0.1577)\n",
      "Epoch: [0][1650/5000]----Batch Time 0.691 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.4799 (0.4325)----Adv. Loss 4.3205 (3.2903)----Disc. Loss 0.0262 (0.1537)\n",
      "Epoch: [0][1700/5000]----Batch Time 0.696 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.4714 (0.4317)----Adv. Loss 4.3771 (3.3217)----Disc. Loss 0.0240 (0.1499)\n",
      "Epoch: [0][1750/5000]----Batch Time 0.684 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.3779 (0.4317)----Adv. Loss 4.4155 (3.3520)----Disc. Loss 0.0229 (0.1463)\n",
      "Epoch: [0][1800/5000]----Batch Time 0.687 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.3736 (0.4312)----Adv. Loss 4.4417 (3.3815)----Disc. Loss 0.0234 (0.1429)\n",
      "Epoch: [0][1850/5000]----Batch Time 0.689 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.4678 (0.4314)----Adv. Loss 4.4596 (3.4102)----Disc. Loss 0.0218 (0.1396)\n",
      "Epoch: [0][1900/5000]----Batch Time 0.690 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.3040 (0.4310)----Adv. Loss 4.4835 (3.4380)----Disc. Loss 0.0217 (0.1365)\n",
      "Epoch: [0][1950/5000]----Batch Time 0.706 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.3866 (0.4310)----Adv. Loss 4.4675 (3.4653)----Disc. Loss 0.0221 (0.1336)\n",
      "Epoch: [0][2000/5000]----Batch Time 0.691 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.5975 (0.4310)----Adv. Loss 4.5259 (3.4920)----Disc. Loss 0.0211 (0.1307)\n",
      "Epoch: [0][2050/5000]----Batch Time 0.683 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.3997 (0.4303)----Adv. Loss 4.5890 (3.5180)----Disc. Loss 0.0199 (0.1280)\n",
      "Epoch: [0][2100/5000]----Batch Time 0.690 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.2108 (0.4302)----Adv. Loss 4.5975 (3.5432)----Disc. Loss 0.0187 (0.1255)\n",
      "Epoch: [0][2150/5000]----Batch Time 0.699 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.3310 (0.4299)----Adv. Loss 4.6505 (3.5680)----Disc. Loss 0.0172 (0.1230)\n",
      "Epoch: [0][2200/5000]----Batch Time 0.687 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.3661 (0.4295)----Adv. Loss 4.6107 (3.5921)----Disc. Loss 0.0189 (0.1206)\n",
      "Epoch: [0][2250/5000]----Batch Time 0.693 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.4258 (0.4292)----Adv. Loss 4.7010 (3.6160)----Disc. Loss 0.0173 (0.1183)\n",
      "Epoch: [0][2300/5000]----Batch Time 0.693 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.4204 (0.4283)----Adv. Loss 4.6999 (3.6392)----Disc. Loss 0.0167 (0.1162)\n",
      "Epoch: [0][2350/5000]----Batch Time 0.685 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.4347 (0.4285)----Adv. Loss 4.6812 (3.6619)----Disc. Loss 0.0188 (0.1141)\n",
      "Epoch: [0][2400/5000]----Batch Time 0.689 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.3077 (0.4284)----Adv. Loss 4.7702 (3.6842)----Disc. Loss 0.0158 (0.1120)\n",
      "Epoch: [0][2450/5000]----Batch Time 0.695 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.4475 (0.4279)----Adv. Loss 4.7502 (3.7059)----Disc. Loss 0.0162 (0.1101)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][2500/5000]----Batch Time 0.689 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.4618 (0.4282)----Adv. Loss 4.7620 (3.7272)----Disc. Loss 0.0160 (0.1082)\n",
      "Epoch: [0][2550/5000]----Batch Time 0.685 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.3906 (0.4277)----Adv. Loss 4.8015 (3.7482)----Disc. Loss 0.0159 (0.1064)\n",
      "Epoch: [0][2600/5000]----Batch Time 0.685 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.4216 (0.4274)----Adv. Loss 4.7786 (3.7687)----Disc. Loss 0.0166 (0.1046)\n",
      "Epoch: [0][2650/5000]----Batch Time 0.693 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.3450 (0.4275)----Adv. Loss 4.8557 (3.7888)----Disc. Loss 0.0144 (0.1030)\n",
      "Epoch: [0][2700/5000]----Batch Time 0.702 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.5169 (0.4275)----Adv. Loss 4.8790 (3.8084)----Disc. Loss 0.0142 (0.1013)\n",
      "Epoch: [0][2750/5000]----Batch Time 0.694 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.2882 (0.4272)----Adv. Loss 4.9129 (3.8278)----Disc. Loss 0.0140 (0.0997)\n",
      "Epoch: [0][2800/5000]----Batch Time 0.687 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.5175 (0.4274)----Adv. Loss 4.8744 (3.8468)----Disc. Loss 0.0157 (0.0982)\n",
      "Epoch: [0][2850/5000]----Batch Time 0.693 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.4727 (0.4275)----Adv. Loss 4.9032 (3.8654)----Disc. Loss 0.0149 (0.0968)\n",
      "Epoch: [0][2900/5000]----Batch Time 0.687 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.5155 (0.4278)----Adv. Loss 4.9206 (3.8837)----Disc. Loss 0.0145 (0.0953)\n",
      "Epoch: [0][2950/5000]----Batch Time 0.693 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.5051 (0.4277)----Adv. Loss 4.9540 (3.9019)----Disc. Loss 0.0135 (0.0939)\n",
      "Epoch: [0][3000/5000]----Batch Time 0.689 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.3850 (0.4277)----Adv. Loss 5.0009 (3.9196)----Disc. Loss 0.0125 (0.0926)\n",
      "Epoch: [0][3050/5000]----Batch Time 0.699 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.4535 (0.4273)----Adv. Loss 4.9737 (3.9371)----Disc. Loss 0.0140 (0.0913)\n",
      "Epoch: [0][3100/5000]----Batch Time 0.689 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.4537 (0.4276)----Adv. Loss 5.0464 (3.9541)----Disc. Loss 0.0120 (0.0900)\n",
      "Epoch: [0][3150/5000]----Batch Time 0.692 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.5828 (0.4274)----Adv. Loss 5.0152 (3.9711)----Disc. Loss 0.0129 (0.0888)\n",
      "Epoch: [0][3200/5000]----Batch Time 0.693 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.5037 (0.4271)----Adv. Loss 5.0108 (3.9877)----Disc. Loss 0.0128 (0.0876)\n",
      "Epoch: [0][3250/5000]----Batch Time 0.692 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.5271 (0.4269)----Adv. Loss 5.0654 (4.0040)----Disc. Loss 0.0126 (0.0865)\n",
      "Epoch: [0][3300/5000]----Batch Time 0.683 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.3344 (0.4264)----Adv. Loss 5.0837 (4.0202)----Disc. Loss 0.0124 (0.0853)\n",
      "Epoch: [0][3350/5000]----Batch Time 0.688 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.5700 (0.4262)----Adv. Loss 5.1104 (4.0361)----Disc. Loss 0.0112 (0.0842)\n",
      "Epoch: [0][3400/5000]----Batch Time 0.687 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.3615 (0.4265)----Adv. Loss 5.0919 (4.0517)----Disc. Loss 0.0113 (0.0832)\n",
      "Epoch: [0][3450/5000]----Batch Time 0.703 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.3877 (0.4265)----Adv. Loss 5.1221 (4.0671)----Disc. Loss 0.0112 (0.0821)\n",
      "Epoch: [0][3500/5000]----Batch Time 0.691 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.5110 (0.4266)----Adv. Loss 5.1221 (4.0823)----Disc. Loss 0.0116 (0.0811)\n",
      "Epoch: [0][3550/5000]----Batch Time 0.692 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.4415 (0.4266)----Adv. Loss 5.1197 (4.0971)----Disc. Loss 0.0118 (0.0801)\n",
      "Epoch: [0][3600/5000]----Batch Time 0.694 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.5165 (0.4265)----Adv. Loss 5.1858 (4.1120)----Disc. Loss 0.0104 (0.0792)\n",
      "Epoch: [0][3650/5000]----Batch Time 0.703 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.4271 (0.4268)----Adv. Loss 5.1856 (4.1266)----Disc. Loss 0.0111 (0.0782)\n",
      "Epoch: [0][3700/5000]----Batch Time 0.706 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.4616 (0.4267)----Adv. Loss 5.2288 (4.1410)----Disc. Loss 0.0100 (0.0773)\n",
      "Epoch: [0][3750/5000]----Batch Time 0.691 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.2780 (0.4268)----Adv. Loss 5.1838 (4.1552)----Disc. Loss 0.0108 (0.0764)\n",
      "Epoch: [0][3800/5000]----Batch Time 0.689 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.4480 (0.4268)----Adv. Loss 5.2520 (4.1692)----Disc. Loss 0.0097 (0.0756)\n",
      "Epoch: [0][3850/5000]----Batch Time 0.689 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.4575 (0.4268)----Adv. Loss 5.2146 (4.1830)----Disc. Loss 0.0107 (0.0747)\n",
      "Epoch: [0][3900/5000]----Batch Time 0.690 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.2377 (0.4265)----Adv. Loss 5.2786 (4.1966)----Disc. Loss 0.0093 (0.0739)\n",
      "Epoch: [0][3950/5000]----Batch Time 0.693 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.3324 (0.4264)----Adv. Loss 5.2918 (4.2100)----Disc. Loss 0.0095 (0.0731)\n",
      "Epoch: [0][4000/5000]----Batch Time 0.693 (0.694)----Data Time 0.000 (0.001)----Cont. Loss 0.3400 (0.4262)----Adv. Loss 5.2846 (4.2234)----Disc. Loss 0.0097 (0.0723)\n",
      "Epoch: [0][4050/5000]----Batch Time 0.695 (0.693)----Data Time 0.000 (0.001)----Cont. Loss 0.3037 (0.4261)----Adv. Loss 5.3132 (4.2365)----Disc. Loss 0.0091 (0.0715)\n",
      "Epoch: [0][4100/5000]----Batch Time 0.690 (0.693)----Data Time 0.000 (0.001)----Cont. Loss 0.3981 (0.4260)----Adv. Loss 5.3357 (4.2495)----Disc. Loss 0.0091 (0.0708)\n",
      "Epoch: [0][4150/5000]----Batch Time 0.687 (0.693)----Data Time 0.000 (0.001)----Cont. Loss 0.4384 (0.4260)----Adv. Loss 5.3378 (4.2623)----Disc. Loss 0.0090 (0.0700)\n",
      "Epoch: [0][4200/5000]----Batch Time 0.690 (0.693)----Data Time 0.000 (0.001)----Cont. Loss 0.4072 (0.4256)----Adv. Loss 5.3346 (4.2749)----Disc. Loss 0.0089 (0.0693)\n",
      "Epoch: [0][4250/5000]----Batch Time 0.693 (0.693)----Data Time 0.000 (0.001)----Cont. Loss 0.3849 (0.4254)----Adv. Loss 5.3592 (4.2874)----Disc. Loss 0.0090 (0.0686)\n",
      "Epoch: [0][4300/5000]----Batch Time 0.692 (0.693)----Data Time 0.000 (0.001)----Cont. Loss 0.4702 (0.4251)----Adv. Loss 5.3250 (4.2998)----Disc. Loss 0.0098 (0.0679)\n",
      "Epoch: [0][4350/5000]----Batch Time 0.689 (0.693)----Data Time 0.000 (0.001)----Cont. Loss 0.5341 (0.4250)----Adv. Loss 5.3671 (4.3121)----Disc. Loss 0.0099 (0.0672)\n",
      "Epoch: [0][4400/5000]----Batch Time 0.694 (0.693)----Data Time 0.000 (0.001)----Cont. Loss 0.4639 (0.4250)----Adv. Loss 5.3947 (4.3241)----Disc. Loss 0.0087 (0.0666)\n",
      "Epoch: [0][4450/5000]----Batch Time 0.690 (0.693)----Data Time 0.000 (0.001)----Cont. Loss 0.2982 (0.4246)----Adv. Loss 5.3680 (4.3361)----Disc. Loss 0.0095 (0.0659)\n",
      "Epoch: [0][4500/5000]----Batch Time 0.691 (0.693)----Data Time 0.000 (0.001)----Cont. Loss 0.5048 (0.4247)----Adv. Loss 5.3635 (4.3478)----Disc. Loss 0.0094 (0.0653)\n",
      "Epoch: [0][4550/5000]----Batch Time 0.689 (0.693)----Data Time 0.000 (0.001)----Cont. Loss 0.3574 (0.4244)----Adv. Loss 5.4395 (4.3595)----Disc. Loss 0.0086 (0.0647)\n",
      "Epoch: [0][4600/5000]----Batch Time 0.689 (0.693)----Data Time 0.000 (0.001)----Cont. Loss 0.3009 (0.4244)----Adv. Loss 5.4594 (4.3710)----Disc. Loss 0.0082 (0.0641)\n",
      "Epoch: [0][4650/5000]----Batch Time 0.687 (0.693)----Data Time 0.000 (0.001)----Cont. Loss 0.4227 (0.4244)----Adv. Loss 5.4405 (4.3824)----Disc. Loss 0.0082 (0.0635)\n",
      "Epoch: [0][4700/5000]----Batch Time 0.693 (0.693)----Data Time 0.000 (0.001)----Cont. Loss 0.3454 (0.4243)----Adv. Loss 5.4462 (4.3937)----Disc. Loss 0.0085 (0.0629)\n",
      "Epoch: [0][4750/5000]----Batch Time 0.694 (0.693)----Data Time 0.000 (0.001)----Cont. Loss 0.4254 (0.4241)----Adv. Loss 5.4284 (4.4049)----Disc. Loss 0.0086 (0.0623)\n",
      "Epoch: [0][4800/5000]----Batch Time 0.701 (0.693)----Data Time 0.000 (0.001)----Cont. Loss 0.6139 (0.4239)----Adv. Loss 5.4831 (4.4160)----Disc. Loss 0.0080 (0.0617)\n",
      "Epoch: [0][4850/5000]----Batch Time 0.686 (0.693)----Data Time 0.000 (0.001)----Cont. Loss 0.4145 (0.4240)----Adv. Loss 5.4939 (4.4269)----Disc. Loss 0.0081 (0.0612)\n",
      "Epoch: [0][4900/5000]----Batch Time 0.691 (0.693)----Data Time 0.000 (0.001)----Cont. Loss 0.5127 (0.4241)----Adv. Loss 5.4811 (4.4378)----Disc. Loss 0.0083 (0.0606)\n",
      "Epoch: [0][4950/5000]----Batch Time 0.693 (0.693)----Data Time 0.001 (0.001)----Cont. Loss 0.2658 (0.4243)----Adv. Loss 5.5251 (4.4485)----Disc. Loss 0.0077 (0.0601)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/5000]----Batch Time 2.494 (2.494)----Data Time 1.740 (1.740)----Cont. Loss 0.2939 (0.2939)----Adv. Loss 5.5233 (5.5233)----Disc. Loss 0.0074 (0.0074)\n",
      "Epoch: [1][50/5000]----Batch Time 0.686 (0.732)----Data Time 0.000 (0.035)----Cont. Loss 0.4179 (0.4172)----Adv. Loss 5.5546 (5.5296)----Disc. Loss 0.0074 (0.0076)\n",
      "Epoch: [1][100/5000]----Batch Time 0.685 (0.710)----Data Time 0.000 (0.018)----Cont. Loss 0.4462 (0.4146)----Adv. Loss 5.5230 (5.5293)----Disc. Loss 0.0076 (0.0076)\n",
      "Epoch: [1][150/5000]----Batch Time 0.685 (0.703)----Data Time 0.000 (0.012)----Cont. Loss 0.4878 (0.4117)----Adv. Loss 5.5455 (5.5319)----Disc. Loss 0.0074 (0.0076)\n",
      "Epoch: [1][200/5000]----Batch Time 0.691 (0.699)----Data Time 0.000 (0.009)----Cont. Loss 0.2407 (0.4113)----Adv. Loss 5.5685 (5.5358)----Disc. Loss 0.0069 (0.0076)\n",
      "Epoch: [1][250/5000]----Batch Time 0.681 (0.697)----Data Time 0.000 (0.007)----Cont. Loss 0.4119 (0.4126)----Adv. Loss 5.5607 (5.5416)----Disc. Loss 0.0073 (0.0076)\n",
      "Epoch: [1][300/5000]----Batch Time 0.687 (0.695)----Data Time 0.000 (0.006)----Cont. Loss 0.3908 (0.4084)----Adv. Loss 5.5960 (5.5467)----Disc. Loss 0.0069 (0.0075)\n",
      "Epoch: [1][350/5000]----Batch Time 0.691 (0.694)----Data Time 0.000 (0.005)----Cont. Loss 0.3824 (0.4106)----Adv. Loss 5.6012 (5.5512)----Disc. Loss 0.0071 (0.0075)\n",
      "Epoch: [1][400/5000]----Batch Time 0.685 (0.693)----Data Time 0.000 (0.005)----Cont. Loss 0.5382 (0.4131)----Adv. Loss 5.6056 (5.5558)----Disc. Loss 0.0070 (0.0075)\n",
      "Epoch: [1][450/5000]----Batch Time 0.691 (0.693)----Data Time 0.000 (0.004)----Cont. Loss 0.3924 (0.4143)----Adv. Loss 5.6031 (5.5609)----Disc. Loss 0.0070 (0.0074)\n",
      "Epoch: [1][500/5000]----Batch Time 0.685 (0.692)----Data Time 0.000 (0.004)----Cont. Loss 0.5192 (0.4165)----Adv. Loss 5.6040 (5.5656)----Disc. Loss 0.0073 (0.0074)\n",
      "Epoch: [1][550/5000]----Batch Time 0.686 (0.691)----Data Time 0.000 (0.004)----Cont. Loss 0.4047 (0.4174)----Adv. Loss 5.6398 (5.5707)----Disc. Loss 0.0065 (0.0074)\n",
      "Epoch: [1][600/5000]----Batch Time 0.685 (0.691)----Data Time 0.000 (0.003)----Cont. Loss 0.4813 (0.4188)----Adv. Loss 5.6250 (5.5755)----Disc. Loss 0.0074 (0.0073)\n",
      "Epoch: [1][650/5000]----Batch Time 0.691 (0.691)----Data Time 0.000 (0.003)----Cont. Loss 0.5581 (0.4198)----Adv. Loss 5.6159 (5.5801)----Disc. Loss 0.0083 (0.0073)\n",
      "Epoch: [1][700/5000]----Batch Time 0.683 (0.691)----Data Time 0.000 (0.003)----Cont. Loss 0.4937 (0.4196)----Adv. Loss 5.6598 (5.5850)----Disc. Loss 0.0069 (0.0073)\n",
      "Epoch: [1][750/5000]----Batch Time 0.690 (0.690)----Data Time 0.000 (0.003)----Cont. Loss 0.5617 (0.4199)----Adv. Loss 5.6703 (5.5897)----Disc. Loss 0.0067 (0.0072)\n",
      "Epoch: [1][800/5000]----Batch Time 0.691 (0.690)----Data Time 0.000 (0.003)----Cont. Loss 0.3887 (0.4210)----Adv. Loss 5.6573 (5.5945)----Disc. Loss 0.0068 (0.0072)\n",
      "Epoch: [1][850/5000]----Batch Time 0.687 (0.690)----Data Time 0.000 (0.002)----Cont. Loss 0.6091 (0.4214)----Adv. Loss 5.6655 (5.5992)----Disc. Loss 0.0066 (0.0072)\n",
      "Epoch: [1][900/5000]----Batch Time 0.690 (0.690)----Data Time 0.000 (0.002)----Cont. Loss 0.2739 (0.4210)----Adv. Loss 5.7163 (5.6040)----Disc. Loss 0.0060 (0.0071)\n",
      "Epoch: [1][950/5000]----Batch Time 0.687 (0.692)----Data Time 0.000 (0.002)----Cont. Loss 0.5076 (0.4208)----Adv. Loss 5.6750 (5.6088)----Disc. Loss 0.0065 (0.0071)\n",
      "Epoch: [1][1000/5000]----Batch Time 0.893 (0.695)----Data Time 0.000 (0.002)----Cont. Loss 0.2978 (0.4198)----Adv. Loss 5.6964 (5.6134)----Disc. Loss 0.0062 (0.0071)\n",
      "Epoch: [1][1050/5000]----Batch Time 1.266 (0.701)----Data Time 0.001 (0.002)----Cont. Loss 0.3360 (0.4196)----Adv. Loss 5.7218 (5.6182)----Disc. Loss 0.0060 (0.0070)\n",
      "Epoch: [1][1100/5000]----Batch Time 0.713 (0.715)----Data Time 0.001 (0.002)----Cont. Loss 0.5468 (0.4191)----Adv. Loss 5.7117 (5.6230)----Disc. Loss 0.0064 (0.0070)\n",
      "Epoch: [1][1150/5000]----Batch Time 0.953 (0.721)----Data Time 0.005 (0.002)----Cont. Loss 0.2239 (0.4180)----Adv. Loss 5.7587 (5.6277)----Disc. Loss 0.0060 (0.0070)\n",
      "Epoch: [1][1200/5000]----Batch Time 0.919 (0.729)----Data Time 0.000 (0.002)----Cont. Loss 0.4150 (0.4181)----Adv. Loss 5.7233 (5.6322)----Disc. Loss 0.0067 (0.0069)\n",
      "Epoch: [1][1250/5000]----Batch Time 0.801 (0.736)----Data Time 0.000 (0.002)----Cont. Loss 0.2893 (0.4164)----Adv. Loss 5.7539 (5.6369)----Disc. Loss 0.0059 (0.0069)\n",
      "Epoch: [1][1300/5000]----Batch Time 0.694 (0.742)----Data Time 0.000 (0.002)----Cont. Loss 0.4976 (0.4171)----Adv. Loss 5.7283 (5.6411)----Disc. Loss 0.0070 (0.0069)\n",
      "Epoch: [1][1350/5000]----Batch Time 0.734 (0.750)----Data Time 0.006 (0.002)----Cont. Loss 0.3732 (0.4174)----Adv. Loss 5.7761 (5.6456)----Disc. Loss 0.0061 (0.0068)\n",
      "Epoch: [1][1400/5000]----Batch Time 0.797 (0.755)----Data Time 0.002 (0.002)----Cont. Loss 0.4645 (0.4176)----Adv. Loss 5.8102 (5.6501)----Disc. Loss 0.0055 (0.0068)\n",
      "Epoch: [1][1450/5000]----Batch Time 0.862 (0.762)----Data Time 0.006 (0.003)----Cont. Loss 0.4589 (0.4169)----Adv. Loss 5.7758 (5.6544)----Disc. Loss 0.0060 (0.0068)\n",
      "Epoch: [1][1500/5000]----Batch Time 0.834 (0.765)----Data Time 0.000 (0.003)----Cont. Loss 0.3849 (0.4168)----Adv. Loss 5.7775 (5.6588)----Disc. Loss 0.0058 (0.0068)\n",
      "Epoch: [1][1550/5000]----Batch Time 4.056 (0.771)----Data Time 0.001 (0.003)----Cont. Loss 0.2155 (0.4163)----Adv. Loss 5.8270 (5.6631)----Disc. Loss 0.0059 (0.0067)\n",
      "Epoch: [1][1600/5000]----Batch Time 0.697 (0.776)----Data Time 0.001 (0.003)----Cont. Loss 0.3693 (0.4157)----Adv. Loss 5.8208 (5.6675)----Disc. Loss 0.0056 (0.0067)\n",
      "Epoch: [1][1650/5000]----Batch Time 0.737 (0.778)----Data Time 0.005 (0.003)----Cont. Loss 0.3720 (0.4152)----Adv. Loss 5.8093 (5.6719)----Disc. Loss 0.0058 (0.0067)\n",
      "Epoch: [1][1700/5000]----Batch Time 0.700 (0.780)----Data Time 0.001 (0.003)----Cont. Loss 0.4232 (0.4151)----Adv. Loss 5.8384 (5.6763)----Disc. Loss 0.0057 (0.0066)\n",
      "Epoch: [1][1750/5000]----Batch Time 0.865 (0.784)----Data Time 0.000 (0.003)----Cont. Loss 0.3959 (0.4151)----Adv. Loss 5.8500 (5.6806)----Disc. Loss 0.0054 (0.0066)\n",
      "Epoch: [1][1800/5000]----Batch Time 0.958 (0.790)----Data Time 0.008 (0.003)----Cont. Loss 0.4989 (0.4154)----Adv. Loss 5.8449 (5.6849)----Disc. Loss 0.0053 (0.0066)\n",
      "Epoch: [1][1850/5000]----Batch Time 0.729 (0.791)----Data Time 0.001 (0.003)----Cont. Loss 0.4605 (0.4157)----Adv. Loss 5.8319 (5.6890)----Disc. Loss 0.0060 (0.0066)\n",
      "Epoch: [1][1900/5000]----Batch Time 0.782 (0.792)----Data Time 0.001 (0.003)----Cont. Loss 0.3432 (0.4159)----Adv. Loss 5.8821 (5.6933)----Disc. Loss 0.0052 (0.0065)\n",
      "Epoch: [1][1950/5000]----Batch Time 0.884 (0.795)----Data Time 0.000 (0.003)----Cont. Loss 0.3692 (0.4165)----Adv. Loss 5.8445 (5.6974)----Disc. Loss 0.0054 (0.0065)\n",
      "Epoch: [1][2000/5000]----Batch Time 0.730 (0.797)----Data Time 0.002 (0.003)----Cont. Loss 0.3101 (0.4168)----Adv. Loss 5.8659 (5.7016)----Disc. Loss 0.0055 (0.0065)\n",
      "Epoch: [1][2050/5000]----Batch Time 0.726 (0.800)----Data Time 0.001 (0.003)----Cont. Loss 0.4270 (0.4168)----Adv. Loss 5.8677 (5.7058)----Disc. Loss 0.0058 (0.0065)\n",
      "Epoch: [1][2100/5000]----Batch Time 0.937 (0.801)----Data Time 0.004 (0.003)----Cont. Loss 0.4749 (0.4165)----Adv. Loss 5.9051 (5.7100)----Disc. Loss 0.0051 (0.0064)\n",
      "Epoch: [1][2150/5000]----Batch Time 1.077 (0.805)----Data Time 0.015 (0.003)----Cont. Loss 0.3199 (0.4164)----Adv. Loss 5.9107 (5.7140)----Disc. Loss 0.0053 (0.0064)\n",
      "Epoch: [1][2200/5000]----Batch Time 0.742 (0.808)----Data Time 0.001 (0.003)----Cont. Loss 0.4790 (0.4166)----Adv. Loss 5.8713 (5.7181)----Disc. Loss 0.0057 (0.0064)\n",
      "Epoch: [1][2250/5000]----Batch Time 0.854 (0.810)----Data Time 0.001 (0.003)----Cont. Loss 0.4250 (0.4164)----Adv. Loss 5.9007 (5.7221)----Disc. Loss 0.0053 (0.0064)\n",
      "Epoch: [1][2300/5000]----Batch Time 3.437 (0.817)----Data Time 0.009 (0.003)----Cont. Loss 0.3724 (0.4164)----Adv. Loss 5.8985 (5.7261)----Disc. Loss 0.0058 (0.0063)\n",
      "Epoch: [1][2350/5000]----Batch Time 0.861 (0.818)----Data Time 0.001 (0.003)----Cont. Loss 0.5110 (0.4164)----Adv. Loss 5.9149 (5.7302)----Disc. Loss 0.0053 (0.0063)\n",
      "Epoch: [1][2400/5000]----Batch Time 0.820 (0.820)----Data Time 0.001 (0.003)----Cont. Loss 0.3650 (0.4162)----Adv. Loss 5.9142 (5.7342)----Disc. Loss 0.0052 (0.0063)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaoxiong.yang/DeepLearning/案例7/code/models.py:333: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  out_channels = (n_channels if i is 0 else in_channels * 2) if i % 2 is 0 else in_channels\n",
      "/Users/chaoxiong.yang/DeepLearning/案例7/code/models.py:333: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  out_channels = (n_channels if i is 0 else in_channels * 2) if i % 2 is 0 else in_channels\n",
      "/Users/chaoxiong.yang/DeepLearning/案例7/code/models.py:336: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  stride=1 if i % 2 is 0 else 2, batch_norm=i is not 0,\n",
      "/Users/chaoxiong.yang/DeepLearning/案例7/code/models.py:336: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  stride=1 if i % 2 is 0 else 2, batch_norm=i is not 0,\n",
      "/Users/chaoxiong.yang/DeepLearning/案例7/code/models.py:333: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  out_channels = (n_channels if i is 0 else in_channels * 2) if i % 2 is 0 else in_channels\n",
      "/Users/chaoxiong.yang/DeepLearning/案例7/code/models.py:333: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  out_channels = (n_channels if i is 0 else in_channels * 2) if i % 2 is 0 else in_channels\n",
      "/Users/chaoxiong.yang/DeepLearning/案例7/code/models.py:336: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  stride=1 if i % 2 is 0 else 2, batch_norm=i is not 0,\n",
      "/Users/chaoxiong.yang/DeepLearning/案例7/code/models.py:336: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  stride=1 if i % 2 is 0 else 2, batch_norm=i is not 0,\n",
      "/Users/chaoxiong.yang/DeepLearning/案例7/code/models.py:333: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  out_channels = (n_channels if i is 0 else in_channels * 2) if i % 2 is 0 else in_channels\n",
      "/Users/chaoxiong.yang/DeepLearning/案例7/code/models.py:333: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  out_channels = (n_channels if i is 0 else in_channels * 2) if i % 2 is 0 else in_channels\n",
      "/Users/chaoxiong.yang/DeepLearning/案例7/code/models.py:336: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  stride=1 if i % 2 is 0 else 2, batch_norm=i is not 0,\n",
      "/Users/chaoxiong.yang/DeepLearning/案例7/code/models.py:336: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  stride=1 if i % 2 is 0 else 2, batch_norm=i is not 0,\n",
      "/Users/chaoxiong.yang/DeepLearning/案例7/code/models.py:333: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  out_channels = (n_channels if i is 0 else in_channels * 2) if i % 2 is 0 else in_channels\n",
      "/Users/chaoxiong.yang/DeepLearning/案例7/code/models.py:333: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  out_channels = (n_channels if i is 0 else in_channels * 2) if i % 2 is 0 else in_channels\n",
      "/Users/chaoxiong.yang/DeepLearning/案例7/code/models.py:336: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  stride=1 if i % 2 is 0 else 2, batch_norm=i is not 0,\n",
      "/Users/chaoxiong.yang/DeepLearning/案例7/code/models.py:336: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  stride=1 if i % 2 is 0 else 2, batch_norm=i is not 0,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     adjust_learning_rate(optimizer_d, \u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# One epoch's training\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m      \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtruncated_vgg19\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncated_vgg19\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcontent_loss_criterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_loss_criterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m      \u001b[49m\u001b[43madversarial_loss_criterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madversarial_loss_criterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptimizer_g\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_g\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptimizer_d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m      \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_freq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Save checkpoint\u001b[39;00m\n\u001b[1;32m     21\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch,\n\u001b[1;32m     22\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerator\u001b[39m\u001b[38;5;124m'\u001b[39m: generator,\n\u001b[1;32m     23\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiscriminator\u001b[39m\u001b[38;5;124m'\u001b[39m: discriminator,\n\u001b[1;32m     24\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_g\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer_g,\n\u001b[1;32m     25\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_d\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer_d},\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_srgan.pth.tar\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/DeepLearning/案例7/code/solver.py:36\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, generator, discriminator, truncated_vgg19, content_loss_criterion, adversarial_loss_criterion, optimizer_g, optimizer_d, epoch, device, beta, print_freq)\u001b[0m\n\u001b[1;32m     33\u001b[0m sr_imgs_in_vgg_space \u001b[38;5;241m=\u001b[39m truncated_vgg19(sr_imgs)\n\u001b[1;32m     34\u001b[0m hr_imgs_in_vgg_space \u001b[38;5;241m=\u001b[39m truncated_vgg19(hr_imgs)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m---> 36\u001b[0m sr_discriminated \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43msr_imgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m content_loss \u001b[38;5;241m=\u001b[39m content_loss_criterion(sr_imgs_in_vgg_space, hr_imgs_in_vgg_space)\n\u001b[1;32m     39\u001b[0m adversarial_loss \u001b[38;5;241m=\u001b[39m adversarial_loss_criterion(sr_discriminated, torch\u001b[38;5;241m.\u001b[39mones_like(sr_discriminated))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/DeepLearning/案例7/code/models.py:391\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, imgs)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_attention:\n\u001b[1;32m    390\u001b[0m     output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCA2(output)\n\u001b[0;32m--> 391\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv9(output) \u001b[38;5;66;03m#512->128\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# if self.self_attention:\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m#     output=self.self_attention_block_1(output)\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;66;03m# if self.channel_attention:\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m#     output=self.CA3(output)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/DeepLearning/案例7/code/models.py:101\u001b[0m, in \u001b[0;36mConvolutionalBlock.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m    Forward propagation.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    :param input: input images, a tensor of size (N, in_channels, w, h)\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    :return: output images, a tensor of size (N, out_channels, w, h)\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (N, out_channels, w, h)\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2452\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Epochs\n",
    "for epoch in range(config.start_epoch, config.epochs):\n",
    "    # At the halfway point, reduce learning rate to a tenth\n",
    "    if epoch == int(config.epochs / 2 + 1):\n",
    "        adjust_learning_rate(optimizer_g, 0.1)\n",
    "        adjust_learning_rate(optimizer_d, 0.1)\n",
    "    # One epoch's training\n",
    "    train(train_loader=train_loader,\n",
    "          generator=generator,\n",
    "          discriminator=discriminator,\n",
    "          truncated_vgg19=truncated_vgg19,\n",
    "          content_loss_criterion=content_loss_criterion,\n",
    "          adversarial_loss_criterion=adversarial_loss_criterion,\n",
    "          optimizer_g=optimizer_g,\n",
    "          optimizer_d=optimizer_d,\n",
    "          epoch=epoch,\n",
    "          device=config.device,\n",
    "          beta=config.beta,\n",
    "          print_freq=config.print_freq)\n",
    "    # Save checkpoint\n",
    "    torch.save({'epoch': epoch,\n",
    "                'generator': generator,\n",
    "                'discriminator': discriminator,\n",
    "                'optimizer_g': optimizer_g,\n",
    "                'optimizer_d': optimizer_d},\n",
    "                'checkpoint_srgan.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
